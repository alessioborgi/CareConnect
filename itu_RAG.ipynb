{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b260330d",
   "metadata": {},
   "source": [
    "# Building a simple RAG\n",
    "We have learned that ChatGPT knows very little about ITU. so let's incorporate some knowledge as a RAG!\n",
    "\n",
    "For the sake of simplicity, we use LlamaIndex. But you could also build something manually (as most companies do) based on a custom RAG architecture (i.e., programming the communication between LLM and Vector DB/SQL DB yourself).\n",
    "\n",
    "Either way, we are aiming for a basic setup like this:\n",
    "![Typical RAG pipeline](RAG_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e67dd",
   "metadata": {},
   "source": [
    "## Load external data\n",
    "Using pre-built class for directory reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645d35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81fb6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"preprocessed\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4072917-68e5-4f43-acde-840928a58a52",
   "metadata": {},
   "source": [
    "### Build Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c13665-9833-4980-b57c-a4a00e0797d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# first, load and set openaikey from a txt file I stored it in\n",
    "with open('oaikey.txt') as keyfile:\n",
    "    oaikey = keyfile.read().strip()\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = oaikey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da114147-80a1-435c-a04b-182b71bf602f",
   "metadata": {},
   "source": [
    "### Set up Vector database (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e7a585f-7aa5-4982-8e15-14b8ee410ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cbdeb-4d11-41a0-82ae-7a741849fc35",
   "metadata": {},
   "source": [
    "#### Initite chroma db and collection of vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21917b92-e1f0-4599-b831-e850ccbf9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_client = chromadb.EphemeralClient() # this would only be in RAM\n",
    "chroma_client = chromadb.PersistentClient(path=\"./lecture_db/chroma\") # client stored on disk\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=\"itu_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e1459-9f9a-428d-9589-1125b1994ee9",
   "metadata": {},
   "source": [
    "#### Wrap in LLamaIndex objects for easier handling/compatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "380b782a-b12b-4fdf-b561-87256c3f7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... basically, we are just specifying the storage to be used as the ChromaDB\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b0b8c5-afe5-4307-97cc-cf5d56874e28",
   "metadata": {},
   "source": [
    "#### Specify the embedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23865181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# let's use OpenAI out-of-the box embeddings.\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079f6dd-74e0-4ae9-8b4c-8c09cdd0cbb6",
   "metadata": {},
   "source": [
    "### Start building the database from our config!\n",
    "Note that you could add multiple processing steps here, such as:\n",
    "- Using an [ingestion pipeline](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/) for further preprocessing\n",
    "- [modifying chunk size and overlap](https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies/#chunk-sizes) or introduce specific chunking strategy\n",
    "- and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3023c4-8c2a-415b-b4d9-dc29b4008c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could modify chunk size and overlap like this\n",
    "# Settings.chunk_size = 512\n",
    "# Settings.chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c4a2c61-af88-4cf1-b018-543441f795ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0beef20cfff4267a20fdccafe4e0579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b500e063cb747be9c37837b8a4d4237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "# now let's build an index for the database using pre-built functionality\n",
    "# - Chunk the documents\n",
    "# - Retrieve embeddings for document chunks\n",
    "# - Create nodes in db based on docs/chunks\n",
    "# - Index database for fast retrieval\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fc8e44f-857a-4beb-a606-6db4570ab0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we are loading from disk, uncomment\n",
    "# from llama_index.core import load_index_from_storage\n",
    "# index2 = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd69ab2-b420-4619-bc84-680cbcad4520",
   "metadata": {},
   "source": [
    "#### Test created embedding/chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bccc43-56d4-4be0-ba12-983a4d475381",
   "metadata": {},
   "source": [
    "### Specify LLM-Chat interface\n",
    "Now, we want to build the communication between an LLM and our database that resembles our typical RAG setup:\n",
    "![Typical RAG pipeline](RAG_pipeline.png)\n",
    "\n",
    "\n",
    "\n",
    "Using LlamaIndex, this is deceptively easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa22855-fb91-43fc-8ed1-8a6f079ae722",
   "metadata": {},
   "source": [
    "#### Specify details about retrieval from vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb8ebbaf-0dca-4ef3-9014-6cf12ff03eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Let's do some logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942fa04-2fca-4ada-a3d6-0debe3cc22e6",
   "metadata": {},
   "source": [
    "#### Configure retrieval from VectorDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05262818-6daa-4c83-ad5e-53efc9d38307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this specifies the details for retrieving the k closest elements to the user query\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5, # how many documents should we consider? Let's do 5\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68143899-37d7-493b-a57d-6e4afc382e54",
   "metadata": {},
   "source": [
    "#### Specify the used LLM\n",
    "in this case, we use the OpenAI GPT4o-mini (very performant and cheap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af45f72-606c-4bbb-a1a8-713f8128a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8da2f71e-a769-4b90-9aeb-76b15536498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "# this is an OpenAI wrapper for llama_index\n",
    "llm = OpenAI(model=\"gpt-4o-mini\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a46ed1-247c-4a28-bffd-77cc289a2ad1",
   "metadata": {},
   "source": [
    "#### Specify the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac522d-d197-4e8b-b1f2-ca58a84dca27",
   "metadata": {},
   "source": [
    "Do a simple [RAG-prompt](https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f78d284-0fe2-4031-a821-5df8dabf18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import get_response_synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7bd9400-025d-4f61-9f05-8880fad427cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's specify a prompt similar to what we have learned earlier\n",
    "custom_query = \"\"\"\n",
    "    You are an information chatbot that informs users about the Interdisciplinary Transformation University Austria (ITU) in Linz, Austria. \n",
    "    \n",
    "    Here is the context information:\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    Given the context information, this prompt, and no prior knowledge, answer the query. \n",
    "    The answer must be 100 words or less.\n",
    "    \n",
    "    Query: {query_str}\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eceffcd3-62e5-44d0-9eb0-22e8810a425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this specifies how we utilize the retrieved chunks/text in the response\n",
    "# configure response synthesizer\n",
    "rag_prompt = PromptTemplate(custom_query) # use LLama_index wrapper to create our query\n",
    "\n",
    "# Build response synthesizer:\n",
    "# i.e., object that combines user prompt, retrieved context, and our RAG prompt and sends it to the LLM (GPT-4o-mini)\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm=llm, text_qa_template=rag_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e0ee7-a889-4efa-aa0a-69a71edd246d",
   "metadata": {},
   "source": [
    "#### \"Assemble\" query engine \n",
    "Combine other config into the actual logic that will do the querying for us.\n",
    "\n",
    "Again, we will stick to the basics here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3325fc02-6de5-4104-b295-1a665a146d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "#node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever, # configuration for retrieval of vector chunks\n",
    "    response_synthesizer=response_synthesizer, # config for synthesizing LLM prompt/response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a72d-fcaa-40ab-8c86-38cc8425ba7b",
   "metadata": {},
   "source": [
    "#### Let's prompt away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18163ca0-333b-4607-9172-e7e9fa93e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_response = query_engine.query('Who is the founding president?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "59efd5c5-abee-4c24-921b-bab75f31e764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Founding President of the Interdisciplinary Transformation University Austria (ITU) is Prof. Dr. Stefanie Lindstaedt.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oai_response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6af81473-767c-4d06-8792-197b25cdc1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'542fde3f-28d1-4690-a8a9-dd7722665809': {'file_path': '/home/jovyan/work/preprocessed/en_public-notice_provisional-bylaws-of-itu-idsa.txt',\n",
       "  'file_name': 'en_public-notice_provisional-bylaws-of-itu-idsa.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 36144,\n",
       "  'creation_date': '2024-08-26',\n",
       "  'last_modified_date': '2024-08-26'},\n",
       " '2b55da0d-4516-441d-bc60-44fb958bd8c3': {'file_path': '/home/jovyan/work/preprocessed/en_digital-transformation-university_organization.txt',\n",
       "  'file_name': 'en_digital-transformation-university_organization.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 2374,\n",
       "  'creation_date': '2024-08-26',\n",
       "  'last_modified_date': '2024-08-26'},\n",
       " '81983a30-d6ca-4fce-a39b-6b0ae3c756c5': {'file_path': '/home/jovyan/work/preprocessed/en_public-notice_provisional-bylaws-of-itu-idsa.txt',\n",
       "  'file_name': 'en_public-notice_provisional-bylaws-of-itu-idsa.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 36144,\n",
       "  'creation_date': '2024-08-26',\n",
       "  'last_modified_date': '2024-08-26'}}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which files were used as context information? \n",
    "oai_response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "54fff3f7-f1f4-4aba-8798-63ab83191268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered information from 3 text chunks, for example:\n",
      "Node ID: 2b55da0d-4516-441d-bc60-44fb958bd8c3\n",
      "Document: en_digital-transformation-university_organization.txt\n",
      "Text:\n",
      ":study:careerhome : about : organization\n",
      "© Felix Büchele - IT:U:organization© Lunghammer – TU GrazDipl.-Ing.in Claudia von der Linden, MBA (IMD)Chairwoman of the Founding Convent© Antje Wolm – IT:UProf. Dr.in Stefanie LindstaedtFounding President© Felix Büchele – IT:UGabriele Költringer, EMBAManaging Directorinternational strategic advisory boardfounding conventfounding presidentfounding advisory boardmanaging directorFounding ConventThe Founding Convent is the strategic body of the university during the founding phase. Two of its members were nominated by the province of Upper Austria, three by the Federal Ministry of Education, Science and Research (BMBWF), two by the Federal Ministry for Climate Protection, Environment, Energy, Mobility, Innovation and Technology (BMK), one by the Austrian Science Fund (FWF) and one by the Austrian Research Promotion Agency (FFG).Chairman:Dipl.-Ing.in Claudia von der Linden, MBA (IMD)  Vice-Chairmen: Dr.in Christina Rami-Mark Univ.-Prof. Dipl.-Ing. Dr. Martin HitzMembers:Prof. Dipl.-Ing. Dr.Dr.h.c. Wilfried Eichlseder, (BMBWF)Prof. Dr. Dieter Kranzlmüller, (FWF)Dipl.-Ing. Christopher Lindinger, MAS (BMK)Dipl.-Ing.in Katja Schechtner, MSc (BMK)Dr. Wolfgang Steiner, (Land Oberösterreich)PresidentThe founding president leads and represents the university externally.Managing DirectorThe Managing Director is responsible for the development and management of the administrative area.Founding Advisory BoardThe Founding Advisory Board supports the university, particularly in the development of research priorities and the range of study programs. The rectorates of the University of Linz, the University of Art and Industrial Design Linz, the three technical universities, TU Wien, TU Graz and Montanuniversität Leoben, the University of Applied Sciences Upper Austria and the ÖH have each nominated one member to the advisory board.International Strategic Advisory Board (ISAB)The International Strategic Advisory Board advises the Founding Convent and Founding President on current strategic scientific issues, particularly with regards to setting the strategic course and positioning of the university as well as setting fundamental priorities in research and teaching. In addition, the ISAB provides support with calls for applications, searches, and appointments of professors.:public notice\n"
     ]
    }
   ],
   "source": [
    "# some more details\n",
    "example_node_id = oai_response.source_nodes[1].node_id\n",
    "\n",
    "print(f'Gathered information from {len(oai_response.source_nodes)} text chunks, for example:\\n'\n",
    "      f'Node ID: {example_node_id}\\n'\n",
    "      f'Document: {oai_response.metadata[example_node_id][\"file_name\"]}\\n'\n",
    "      f'Text:\\n{oai_response.source_nodes[1].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7521b-7b6a-4abd-a10d-bca7ba04b207",
   "metadata": {},
   "source": [
    "#### Some other queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f38c9c17-9c79-4f32-8969-f3c81242185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_response2 = query_engine.query('Is there a summer school?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f813ca5-702c-43ff-9151-bedf927ef9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, the Interdisciplinary Transformation University Austria (ITU) is hosting a Summer School in 2024, which has attracted over 200 applicants from 66 countries. The program emphasizes interdisciplinary collaboration and diverse academic backgrounds, with approximately 40 participants expected to be selected for this unique learning opportunity. The review committee is currently evaluating applications, and updates on the selection process will be provided as preparations continue. For more information, you can check the university's website.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oai_response2.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c6de625-2b91-42d4-a0b6-04501c2b48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_response3 = query_engine.query('Are they hiring?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "11c49e1f-24a8-4ec9-8a99-7044ce6d19c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, the Interdisciplinary Transformation University Austria (ITU) is currently hiring. They have up to 12 postdoctoral positions available in the field of Computational X, as well as openings for a LMS Administrator – Full Stack Developer, Content Creator, Financial Controller, Project Controller, and Software Developer. Interested candidates can apply online and are encouraged to submit their applications, including a CV and cover letter. The application deadline for the postdoctoral positions is September 15th, 2024.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oai_response3.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
